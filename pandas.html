<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Pandas Mastery Guide</title>
<style>
  @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;600;700&family=Syne:wght@400;600;700;800&display=swap');

  :root {
    --bg: #0d1117;
    --surface: #161b22;
    --surface2: #1c2128;
    --border: #30363d;
    --accent: #58a6ff;
    --accent2: #3fb950;
    --accent3: #f78166;
    --accent4: #d2a8ff;
    --accent5: #ffa657;
    --text: #e6edf3;
    --text-muted: #8b949e;
    --text-dim: #6e7681;
    --beginner: #3fb950;
    --intermediate: #ffa657;
    --advanced: #f78166;
    --de: #d2a8ff;
    --da: #58a6ff;
  }

  * { margin: 0; padding: 0; box-sizing: border-box; }

  body {
    background: var(--bg);
    color: var(--text);
    font-family: 'Syne', sans-serif;
    min-height: 100vh;
    display: flex;
  }

  /* SIDEBAR */
  #sidebar {
    width: 280px;
    min-width: 280px;
    background: var(--surface);
    border-right: 1px solid var(--border);
    height: 100vh;
    position: sticky;
    top: 0;
    overflow-y: auto;
    display: flex;
    flex-direction: column;
  }

  #sidebar::-webkit-scrollbar { width: 4px; }
  #sidebar::-webkit-scrollbar-thumb { background: var(--border); border-radius: 2px; }

  .sidebar-header {
    padding: 24px 20px 16px;
    border-bottom: 1px solid var(--border);
  }

  .sidebar-header h1 {
    font-size: 18px;
    font-weight: 800;
    background: linear-gradient(135deg, var(--accent), var(--accent4));
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    letter-spacing: -0.5px;
  }

  .sidebar-header p { font-size: 11px; color: var(--text-muted); margin-top: 4px; }

  .search-box {
    padding: 12px 16px;
    border-bottom: 1px solid var(--border);
  }

  .search-box input {
    width: 100%;
    background: var(--surface2);
    border: 1px solid var(--border);
    border-radius: 6px;
    padding: 7px 10px;
    color: var(--text);
    font-family: 'Syne', sans-serif;
    font-size: 13px;
    outline: none;
  }

  .search-box input:focus { border-color: var(--accent); }
  .search-box input::placeholder { color: var(--text-dim); }

  .filter-chips {
    padding: 10px 16px;
    display: flex;
    gap: 6px;
    flex-wrap: wrap;
    border-bottom: 1px solid var(--border);
  }

  .chip {
    padding: 3px 10px;
    border-radius: 20px;
    font-size: 11px;
    font-weight: 600;
    cursor: pointer;
    border: 1px solid transparent;
    transition: all 0.2s;
    font-family: 'Syne', sans-serif;
  }

  .chip.all { border-color: var(--text-muted); color: var(--text-muted); }
  .chip.all.active { background: var(--text-muted); color: var(--bg); }
  .chip.beginner { border-color: var(--beginner); color: var(--beginner); }
  .chip.beginner.active { background: var(--beginner); color: var(--bg); }
  .chip.intermediate { border-color: var(--intermediate); color: var(--intermediate); }
  .chip.intermediate.active { background: var(--intermediate); color: var(--bg); }
  .chip.advanced { border-color: var(--advanced); color: var(--advanced); }
  .chip.advanced.active { background: var(--advanced); color: var(--bg); }
  .chip.de { border-color: var(--de); color: var(--de); }
  .chip.de.active { background: var(--de); color: var(--bg); }
  .chip.da { border-color: var(--da); color: var(--da); }
  .chip.da.active { background: var(--da); color: var(--bg); }

  .nav-section { padding: 8px 0; }

  .nav-category {
    padding: 6px 16px;
    font-size: 10px;
    font-weight: 700;
    color: var(--text-dim);
    letter-spacing: 1px;
    text-transform: uppercase;
    margin-top: 4px;
  }

  .nav-item {
    padding: 7px 16px;
    font-size: 13px;
    color: var(--text-muted);
    cursor: pointer;
    display: flex;
    align-items: center;
    gap: 8px;
    transition: all 0.15s;
    border-left: 2px solid transparent;
  }

  .nav-item:hover { color: var(--text); background: var(--surface2); }
  .nav-item.active { color: var(--accent); border-left-color: var(--accent); background: rgba(88,166,255,0.08); }

  .nav-item .level-dot {
    width: 6px;
    height: 6px;
    border-radius: 50%;
    flex-shrink: 0;
  }

  /* MAIN CONTENT */
  #main {
    flex: 1;
    overflow-y: auto;
    padding: 32px 40px;
    max-width: 900px;
  }

  #main::-webkit-scrollbar { width: 6px; }
  #main::-webkit-scrollbar-thumb { background: var(--border); border-radius: 3px; }

  .section {
    margin-bottom: 48px;
    animation: fadeIn 0.3s ease;
  }

  @keyframes fadeIn { from { opacity: 0; transform: translateY(8px); } to { opacity: 1; transform: translateY(0); } }

  .section-header {
    display: flex;
    align-items: center;
    gap: 12px;
    margin-bottom: 20px;
    padding-bottom: 12px;
    border-bottom: 1px solid var(--border);
  }

  .section-icon { font-size: 22px; }

  .section-title { font-size: 22px; font-weight: 800; letter-spacing: -0.5px; }

  .section-meta { display: flex; gap: 6px; margin-left: auto; flex-shrink: 0; }

  .badge {
    padding: 2px 8px;
    border-radius: 4px;
    font-size: 10px;
    font-weight: 700;
    letter-spacing: 0.5px;
  }

  .badge.beginner { background: rgba(63,185,80,0.15); color: var(--beginner); }
  .badge.intermediate { background: rgba(255,166,87,0.15); color: var(--intermediate); }
  .badge.advanced { background: rgba(247,129,102,0.15); color: var(--advanced); }
  .badge.de { background: rgba(210,168,255,0.15); color: var(--de); }
  .badge.da { background: rgba(88,166,255,0.15); color: var(--da); }

  .description {
    color: var(--text-muted);
    font-size: 14px;
    line-height: 1.7;
    margin-bottom: 18px;
  }

  .concept-card {
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 10px;
    margin-bottom: 16px;
    overflow: hidden;
  }

  .concept-card .card-header {
    padding: 12px 16px;
    display: flex;
    align-items: center;
    justify-content: space-between;
    background: var(--surface2);
    border-bottom: 1px solid var(--border);
  }

  .concept-card .card-title { font-size: 14px; font-weight: 700; color: var(--text); }
  .concept-card .card-desc { font-size: 12px; color: var(--text-muted); margin-top: 2px; }

  .code-block {
    position: relative;
    background: #0d1117;
    padding: 16px;
  }

  .copy-btn {
    position: absolute;
    top: 10px;
    right: 10px;
    background: var(--surface2);
    border: 1px solid var(--border);
    color: var(--text-muted);
    padding: 4px 10px;
    border-radius: 5px;
    font-size: 11px;
    cursor: pointer;
    font-family: 'Syne', sans-serif;
    transition: all 0.2s;
    font-weight: 600;
  }

  .copy-btn:hover { border-color: var(--accent); color: var(--accent); }
  .copy-btn.copied { border-color: var(--accent2); color: var(--accent2); }

  pre {
    font-family: 'JetBrains Mono', monospace;
    font-size: 12.5px;
    line-height: 1.7;
    overflow-x: auto;
    white-space: pre;
    color: var(--text);
  }

  /* Syntax highlighting */
  .kw { color: #ff7b72; }
  .fn { color: #d2a8ff; }
  .str { color: #a5d6ff; }
  .cm { color: #8b949e; font-style: italic; }
  .num { color: #79c0ff; }
  .op { color: #ff7b72; }
  .var { color: #e6edf3; }
  .attr { color: #ffa657; }

  .note-box {
    background: rgba(88,166,255,0.08);
    border: 1px solid rgba(88,166,255,0.25);
    border-radius: 8px;
    padding: 12px 14px;
    font-size: 13px;
    color: var(--accent);
    margin: 12px 0;
  }

  .warn-box {
    background: rgba(255,166,87,0.08);
    border: 1px solid rgba(255,166,87,0.25);
    border-radius: 8px;
    padding: 12px 14px;
    font-size: 13px;
    color: var(--intermediate);
    margin: 12px 0;
  }

  .tip-box {
    background: rgba(63,185,80,0.08);
    border: 1px solid rgba(63,185,80,0.25);
    border-radius: 8px;
    padding: 12px 14px;
    font-size: 13px;
    color: var(--beginner);
    margin: 12px 0;
  }

  .hidden { display: none !important; }

  /* Welcome banner */
  .welcome-banner {
    background: linear-gradient(135deg, rgba(88,166,255,0.12), rgba(210,168,255,0.08));
    border: 1px solid rgba(88,166,255,0.25);
    border-radius: 12px;
    padding: 24px;
    margin-bottom: 32px;
    text-align: center;
  }

  .welcome-banner h2 { font-size: 28px; font-weight: 800; letter-spacing: -1px; margin-bottom: 8px; }
  .welcome-banner p { color: var(--text-muted); font-size: 14px; }

  .stats-row { display: flex; gap: 16px; justify-content: center; margin-top: 16px; flex-wrap: wrap; }

  .stat { text-align: center; }
  .stat .num-big { font-size: 24px; font-weight: 800; color: var(--accent); }
  .stat .label { font-size: 11px; color: var(--text-dim); }

  #progress-bar {
    height: 3px;
    background: linear-gradient(90deg, var(--accent), var(--accent4));
    position: fixed;
    top: 0;
    left: 0;
    width: 0%;
    z-index: 1000;
    transition: width 0.3s;
  }
</style>
</head>
<body>

<div id="progress-bar"></div>

<!-- SIDEBAR -->
<div id="sidebar">
  <div class="sidebar-header">
    <h1>ğŸ¼ Pandas Mastery</h1>
    <p>Beginner â†’ Advanced Â· DE + DA</p>
  </div>
  <div class="search-box">
    <input type="text" id="searchInput" placeholder="Search concepts..." oninput="filterSearch(this.value)">
  </div>
  <div class="filter-chips">
    <span class="chip all active" onclick="filterLevel('all', this)">All</span>
    <span class="chip beginner" onclick="filterLevel('beginner', this)">Beginner</span>
    <span class="chip intermediate" onclick="filterLevel('intermediate', this)">Intermediate</span>
    <span class="chip advanced" onclick="filterLevel('advanced', this)">Advanced</span>
    <span class="chip de" onclick="filterLevel('de', this)">Data Eng</span>
    <span class="chip da" onclick="filterLevel('da', this)">Data Analyst</span>
  </div>
  <div class="nav-section" id="navItems"></div>
</div>

<!-- MAIN CONTENT -->
<div id="main">
  <div class="welcome-banner">
    <h2>Pandas Complete Reference</h2>
    <p>Everything a 4-year experienced Data Engineer & Analyst needs â€” in one place.</p>
    <div class="stats-row">
      <div class="stat"><div class="num-big">17</div><div class="label">Topic Areas</div></div>
      <div class="stat"><div class="num-big">150+</div><div class="label">Code Examples</div></div>
      <div class="stat"><div class="num-big">3</div><div class="label">Levels</div></div>
    </div>
  </div>
  <div id="content"></div>
</div>

<script>
const SECTIONS = [
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. SETUP & DATA STRUCTURES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  {
    id: "setup",
    icon: "ğŸš€",
    title: "Setup & Core Data Structures",
    level: "beginner",
    roles: ["de","da"],
    category: "Foundations",
    description: "Installation, imports, and the two core data structures: Series and DataFrame.",
    cards: [
      {
        title: "Installation & Imports",
        desc: "Standard setup for any pandas project",
        code: `<span class="cm"># Install</span>
pip install pandas numpy

<span class="cm"># Standard imports</span>
<span class="kw">import</span> pandas <span class="kw">as</span> pd
<span class="kw">import</span> numpy <span class="kw">as</span> np

<span class="cm"># Check version</span>
<span class="fn">print</span>(pd.__version__)

<span class="cm"># Display options</span>
pd.set_option(<span class="str">'display.max_columns'</span>, <span class="num">None</span>)
pd.set_option(<span class="str">'display.max_rows'</span>, <span class="num">100</span>)
pd.set_option(<span class="str">'display.float_format'</span>, <span class="str">'{:.2f}'</span>.format)
pd.set_option(<span class="str">'display.max_colwidth'</span>, <span class="num">None</span>)`
      },
      {
        title: "Series â€” 1D labeled array",
        desc: "The building block of DataFrames",
        code: `<span class="cm"># Create Series</span>
s = pd.Series([<span class="num">10</span>, <span class="num">20</span>, <span class="num">30</span>, <span class="num">40</span>])
s = pd.Series([<span class="num">10</span>, <span class="num">20</span>, <span class="num">30</span>], index=[<span class="str">'a'</span>, <span class="str">'b'</span>, <span class="str">'c'</span>])
s = pd.Series({<span class="str">'a'</span>: <span class="num">1</span>, <span class="str">'b'</span>: <span class="num">2</span>, <span class="str">'c'</span>: <span class="num">3</span>})

<span class="cm"># Key attributes</span>
s.values        <span class="cm"># numpy array</span>
s.index         <span class="cm"># Index object</span>
s.dtype         <span class="cm"># data type</span>
s.name          <span class="cm"># series name</span>
s.shape         <span class="cm"># (n,)</span>

<span class="cm"># Access</span>
s[<span class="str">'a'</span>]          <span class="cm"># by label</span>
s.iloc[<span class="num">0</span>]       <span class="cm"># by position</span>
s[s > <span class="num">1</span>]        <span class="cm"># boolean mask</span>`
      },
      {
        title: "DataFrame â€” 2D labeled table",
        desc: "The primary pandas data structure",
        code: `<span class="cm"># From dict</span>
df = pd.DataFrame({
    <span class="str">'name'</span>: [<span class="str">'Alice'</span>, <span class="str">'Bob'</span>, <span class="str">'Charlie'</span>],
    <span class="str">'age'</span>: [<span class="num">25</span>, <span class="num">30</span>, <span class="num">35</span>],
    <span class="str">'salary'</span>: [<span class="num">50000</span>, <span class="num">60000</span>, <span class="num">70000</span>]
})

<span class="cm"># From list of dicts</span>
df = pd.DataFrame([
    {<span class="str">'name'</span>: <span class="str">'Alice'</span>, <span class="str">'age'</span>: <span class="num">25</span>},
    {<span class="str">'name'</span>: <span class="str">'Bob'</span>,   <span class="str">'age'</span>: <span class="num">30</span>}
])

<span class="cm"># Key attributes</span>
df.shape        <span class="cm"># (rows, cols)</span>
df.dtypes       <span class="cm"># column data types</span>
df.columns      <span class="cm"># column names</span>
df.index        <span class="cm"># row index</span>
df.info()       <span class="cm"># summary + memory</span>
df.describe()   <span class="cm"># stats summary</span>
df.head(<span class="num">5</span>)      <span class="cm"># first 5 rows</span>
df.tail(<span class="num">5</span>)      <span class="cm"># last 5 rows</span>
df.sample(<span class="num">5</span>)    <span class="cm"># random 5 rows</span>`
      }
    ]
  },

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. I/O â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  {
    id: "io",
    icon: "ğŸ“‚",
    title: "Reading & Writing Data (I/O)",
    level: "beginner",
    roles: ["de","da"],
    category: "Foundations",
    description: "Read and write CSV, Excel, JSON, SQL, Parquet, and more.",
    cards: [
      {
        title: "CSV",
        desc: "Most common flat file format",
        code: `<span class="cm"># Read CSV</span>
df = pd.read_csv(<span class="str">'data.csv'</span>)
df = pd.read_csv(<span class="str">'data.csv'</span>,
    sep=<span class="str">','</span>,             <span class="cm"># delimiter</span>
    header=<span class="num">0</span>,           <span class="cm"># row to use as header</span>
    index_col=<span class="str">'id'</span>,    <span class="cm"># column as index</span>
    usecols=[<span class="str">'a'</span>,<span class="str">'b'</span>], <span class="cm"># load specific cols</span>
    dtype={<span class="str">'age'</span>: <span class="str">'int32'</span>},
    parse_dates=[<span class="str">'date'</span>],
    nrows=<span class="num">1000</span>,         <span class="cm"># load first N rows</span>
    skiprows=<span class="num">2</span>,          <span class="cm"># skip rows</span>
    na_values=[<span class="str">'N/A'</span>, <span class="str">'--'</span>, <span class="str">''</span>],
    encoding=<span class="str">'utf-8'</span>,
    chunksize=<span class="num">10000</span>     <span class="cm"># read in chunks</span>
)

<span class="cm"># Write CSV</span>
df.to_csv(<span class="str">'out.csv'</span>, index=<span class="kw">False</span>)
df.to_csv(<span class="str">'out.csv'</span>, sep=<span class="str">'|'</span>, index=<span class="kw">False</span>)`
      },
      {
        title: "Excel, JSON, Parquet, SQL",
        desc: "Essential formats for DE/DA",
        code: `<span class="cm"># Excel</span>
df = pd.read_excel(<span class="str">'file.xlsx'</span>, sheet_name=<span class="str">'Sheet1'</span>)
df.to_excel(<span class="str">'out.xlsx'</span>, index=<span class="kw">False</span>, sheet_name=<span class="str">'Results'</span>)

<span class="cm"># JSON</span>
df = pd.read_json(<span class="str">'data.json'</span>)
df = pd.read_json(<span class="str">'data.json'</span>, orient=<span class="str">'records'</span>)
df.to_json(<span class="str">'out.json'</span>, orient=<span class="str">'records'</span>, indent=<span class="num">2</span>)

<span class="cm"># Parquet (best for DE pipelines)</span>
df = pd.read_parquet(<span class="str">'data.parquet'</span>)
df = pd.read_parquet(<span class="str">'data.parquet'</span>, columns=[<span class="str">'col1'</span>, <span class="str">'col2'</span>])
df.to_parquet(<span class="str">'out.parquet'</span>, index=<span class="kw">False</span>, compression=<span class="str">'snappy'</span>)

<span class="cm"># SQL</span>
<span class="kw">import</span> sqlalchemy
engine = sqlalchemy.create_engine(<span class="str">'postgresql://user:pass@host/db'</span>)
df = pd.read_sql(<span class="str">"SELECT * FROM orders WHERE date > '2024-01-01'"</span>, engine)
df = pd.read_sql_table(<span class="str">'orders'</span>, engine)
df.to_sql(<span class="str">'orders_cleaned'</span>, engine, if_exists=<span class="str">'replace'</span>, index=<span class="kw">False</span>)

<span class="cm"># Read multiple CSVs at once (DE pattern)</span>
<span class="kw">import</span> glob
df = pd.concat([pd.read_csv(f) <span class="kw">for</span> f <span class="kw">in</span> glob.glob(<span class="str">'data/*.csv'</span>)], ignore_index=<span class="kw">True</span>)`
      }
    ]
  },

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. SELECTION & INDEXING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  {
    id: "selection",
    icon: "ğŸ¯",
    title: "Selection & Indexing",
    level: "beginner",
    roles: ["de","da"],
    category: "Foundations",
    description: "Precisely select rows and columns using loc, iloc, boolean indexing, and query.",
    cards: [
      {
        title: "Column & Row Selection",
        desc: "Basics of accessing data",
        code: `<span class="cm"># Select column(s)</span>
df[<span class="str">'name'</span>]               <span class="cm"># â†’ Series</span>
df[[<span class="str">'name'</span>, <span class="str">'age'</span>]]      <span class="cm"># â†’ DataFrame</span>

<span class="cm"># loc â€” label-based</span>
df.loc[<span class="num">0</span>]                 <span class="cm"># row by index label</span>
df.loc[<span class="num">0</span>, <span class="str">'name'</span>]        <span class="cm"># single value</span>
df.loc[<span class="num">0</span>:<span class="num">5</span>, <span class="str">'name'</span>:<span class="str">'age'</span>] <span class="cm"># slice (inclusive!)</span>
df.loc[[<span class="num">1</span>,<span class="num">3</span>,<span class="num">5</span>], [<span class="str">'name'</span>,<span class="str">'salary'</span>]]

<span class="cm"># iloc â€” position-based</span>
df.iloc[<span class="num">0</span>]                <span class="cm"># first row</span>
df.iloc[<span class="num">0</span>, <span class="num">1</span>]             <span class="cm"># row 0, col 1</span>
df.iloc[<span class="num">0</span>:<span class="num">5</span>, <span class="num">0</span>:<span class="num">3</span>]         <span class="cm"># slice (exclusive end)</span>
df.iloc[-<span class="num">5</span>:]              <span class="cm"># last 5 rows</span>
df.iloc[:, <span class="num">0</span>:<span class="num">3</span>]           <span class="cm"># all rows, first 3 cols</span>`
      },
      {
        title: "Boolean Filtering",
        desc: "Filter rows by conditions",
        code: `<span class="cm"># Single condition</span>
df[df[<span class="str">'age'</span>] > <span class="num">25</span>]
df[df[<span class="str">'name'</span>] == <span class="str">'Alice'</span>]
df[df[<span class="str">'salary'</span>].isna()]

<span class="cm"># Multiple conditions</span>
df[(df[<span class="str">'age'</span>] > <span class="num">25</span>) & (df[<span class="str">'salary'</span>] > <span class="num">50000</span>)]
df[(df[<span class="str">'city'</span>] == <span class="str">'Mumbai'</span>) | (df[<span class="str">'city'</span>] == <span class="str">'Delhi'</span>)]
df[~(df[<span class="str">'status'</span>] == <span class="str">'inactive'</span>)]  <span class="cm"># NOT condition</span>

<span class="cm"># isin â€” check membership</span>
df[df[<span class="str">'city'</span>].isin([<span class="str">'Mumbai'</span>, <span class="str">'Pune'</span>, <span class="str">'Delhi'</span>])]
df[~df[<span class="str">'city'</span>].isin([<span class="str">'Chennai'</span>])]

<span class="cm"># between â€” range check</span>
df[df[<span class="str">'age'</span>].between(<span class="num">25</span>, <span class="num">35</span>)]

<span class="cm"># query â€” clean readable syntax</span>
df.query(<span class="str">"age > 25 and salary > 50000"</span>)
df.query(<span class="str">"city in ['Mumbai', 'Delhi']"</span>)
city = <span class="str">'Mumbai'</span>
df.query(<span class="str">"city == @city"</span>)   <span class="cm"># use Python variable</span>`
      }
    ]
  },

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. DATA CLEANING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  {
    id: "cleaning",
    icon: "ğŸ§¹",
    title: "Data Cleaning",
    level: "intermediate",
    roles: ["de","da"],
    category: "Data Quality",
    description: "Handle missing values, duplicates, type conversions, and string cleaning.",
    cards: [
      {
        title: "Missing Values",
        desc: "Detect, fill, and drop nulls",
        code: `<span class="cm"># Detect</span>
df.isna()                      <span class="cm"># boolean mask</span>
df.isna().sum()                <span class="cm"># count per column</span>
df.isna().sum() / len(df) * <span class="num">100</span> <span class="cm"># % missing</span>
df.notna()

<span class="cm"># Drop</span>
df.dropna()                    <span class="cm"># drop rows with ANY null</span>
df.dropna(subset=[<span class="str">'col1'</span>, <span class="str">'col2'</span>])  <span class="cm"># specific cols</span>
df.dropna(how=<span class="str">'all'</span>)           <span class="cm"># only if all cols are null</span>
df.dropna(thresh=<span class="num">3</span>)            <span class="cm"># keep rows with â‰¥3 non-null</span>

<span class="cm"># Fill</span>
df.fillna(<span class="num">0</span>)                   <span class="cm"># fill all with 0</span>
df.fillna({<span class="str">'age'</span>: <span class="num">0</span>, <span class="str">'name'</span>: <span class="str">'Unknown'</span>})
df[<span class="str">'col'</span>].fillna(df[<span class="str">'col'</span>].mean())
df[<span class="str">'col'</span>].fillna(df[<span class="str">'col'</span>].median())
df[<span class="str">'col'</span>].fillna(method=<span class="str">'ffill'</span>)  <span class="cm"># forward fill</span>
df[<span class="str">'col'</span>].fillna(method=<span class="str">'bfill'</span>)  <span class="cm"># backward fill</span>

<span class="cm"># Interpolate</span>
df[<span class="str">'col'</span>].interpolate(method=<span class="str">'linear'</span>)`
      },
      {
        title: "Duplicates & Data Types",
        desc: "Remove duplicates and cast types",
        code: `<span class="cm"># Duplicates</span>
df.duplicated()                    <span class="cm"># boolean mask</span>
df.duplicated(subset=[<span class="str">'name'</span>, <span class="str">'date'</span>])
df.drop_duplicates()               <span class="cm"># keep first</span>
df.drop_duplicates(keep=<span class="str">'last'</span>)   <span class="cm"># keep last</span>
df.drop_duplicates(keep=<span class="kw">False</span>)    <span class="cm"># drop ALL dupes</span>

<span class="cm"># Type conversion (critical for DE)</span>
df[<span class="str">'age'</span>] = df[<span class="str">'age'</span>].astype(<span class="str">'int32'</span>)
df[<span class="str">'price'</span>] = df[<span class="str">'price'</span>].astype(<span class="str">'float64'</span>)
df[<span class="str">'flag'</span>] = df[<span class="str">'flag'</span>].astype(<span class="str">'bool'</span>)
df[<span class="str">'cat'</span>] = df[<span class="str">'cat'</span>].astype(<span class="str">'category'</span>)   <span class="cm"># saves memory!</span>

<span class="cm"># Safe conversion</span>
df[<span class="str">'age'</span>] = pd.to_numeric(df[<span class="str">'age'</span>], errors=<span class="str">'coerce'</span>)  <span class="cm"># bad â†’ NaN</span>
df[<span class="str">'date'</span>] = pd.to_datetime(df[<span class="str">'date'</span>], errors=<span class="str">'coerce'</span>)

<span class="cm"># Memory optimization (DE must-know)</span>
<span class="kw">def</span> <span class="fn">optimize_dtypes</span>(df):
    <span class="kw">for</span> col <span class="kw">in</span> df.select_dtypes(<span class="str">'int64'</span>).columns:
        df[col] = pd.to_numeric(df[col], downcast=<span class="str">'integer'</span>)
    <span class="kw">for</span> col <span class="kw">in</span> df.select_dtypes(<span class="str">'float64'</span>).columns:
        df[col] = pd.to_numeric(df[col], downcast=<span class="str">'float'</span>)
    <span class="kw">for</span> col <span class="kw">in</span> df.select_dtypes(<span class="str">'object'</span>).columns:
        <span class="kw">if</span> df[col].nunique() / len(df) < <span class="num">0.5</span>:  <span class="cm"># low cardinality</span>
            df[col] = df[col].astype(<span class="str">'category'</span>)
    <span class="kw">return</span> df`
      },
      {
        title: "String Cleaning",
        desc: "The .str accessor for text operations",
        code: `<span class="cm"># .str accessor â€” vectorized string ops</span>
df[<span class="str">'name'</span>].str.lower()
df[<span class="str">'name'</span>].str.upper()
df[<span class="str">'name'</span>].str.strip()          <span class="cm"># remove whitespace</span>
df[<span class="str">'name'</span>].str.replace(<span class="str">'_'</span>, <span class="str">' '</span>)
df[<span class="str">'name'</span>].str.contains(<span class="str">'manager'</span>, case=<span class="kw">False</span>, na=<span class="kw">False</span>)
df[<span class="str">'name'</span>].str.startswith(<span class="str">'A'</span>)
df[<span class="str">'name'</span>].str.split(<span class="str">','</span>, expand=<span class="kw">True</span>)  <span class="cm"># split into columns</span>
df[<span class="str">'name'</span>].str.extract(<span class="str">r'(\d+)'</span>)        <span class="cm"># regex extract</span>
df[<span class="str">'name'</span>].str.len()                     <span class="cm"># string length</span>
df[<span class="str">'email'</span>].str.split(<span class="str">'@'</span>).str[<span class="num">1</span>]        <span class="cm"># get domain</span>

<span class="cm"># Regex replace</span>
df[<span class="str">'phone'</span>].str.replace(<span class="str">r'[^0-9]'</span>, <span class="str">''</span>, regex=<span class="kw">True</span>)
df[<span class="str">'amount'</span>].str.replace(<span class="str">r'[\$,]'</span>, <span class="str">''</span>, regex=<span class="kw">True</span>).astype(float)`
      }
    ]
  },

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. TRANSFORMATIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  {
    id: "transform",
    icon: "âš™ï¸",
    title: "Transformations",
    level: "intermediate",
    roles: ["de","da"],
    category: "Data Quality",
    description: "Apply functions, create new columns, rename, sort, and reshape data.",
    cards: [
      {
        title: "Apply, Map & Transform",
        desc: "Row/column/element-wise function application",
        code: `<span class="cm"># map â€” element-wise on Series</span>
df[<span class="str">'grade'</span>] = df[<span class="str">'score'</span>].map({<span class="str">'A'</span>:<span class="num">4</span>, <span class="str">'B'</span>:<span class="num">3</span>, <span class="str">'C'</span>:<span class="num">2</span>})
df[<span class="str">'tax'</span>] = df[<span class="str">'income'</span>].map(<span class="kw">lambda</span> x: x * <span class="num">0.3</span>)

<span class="cm"># apply â€” row or column wise on DataFrame</span>
df.apply(<span class="kw">lambda</span> x: x.max() - x.min())          <span class="cm"># column-wise (default axis=0)</span>
df.apply(<span class="kw">lambda</span> row: row[<span class="str">'a'</span>] + row[<span class="str">'b'</span>], axis=<span class="num">1</span>) <span class="cm"># row-wise</span>

<span class="cm"># applymap / map (pandas 2.x) â€” element-wise on DataFrame</span>
df.map(<span class="kw">lambda</span> x: <span class="kw">round</span>(x, <span class="num">2</span>) <span class="kw">if</span> isinstance(x, float) <span class="kw">else</span> x)

<span class="cm"># transform â€” like apply but keeps shape (for groupby)</span>
df[<span class="str">'z_score'</span>] = df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>].transform(
    <span class="kw">lambda</span> x: (x - x.mean()) / x.std()
)

<span class="cm"># np.where â€” vectorized if-else (FAST)</span>
df[<span class="str">'category'</span>] = np.where(df[<span class="str">'age'</span>] >= <span class="num">30</span>, <span class="str">'Senior'</span>, <span class="str">'Junior'</span>)

<span class="cm"># np.select â€” multiple conditions</span>
conditions = [df[<span class="str">'score'</span>] >= <span class="num">90</span>, df[<span class="str">'score'</span>] >= <span class="num">70</span>, df[<span class="str">'score'</span>] >= <span class="num">50</span>]
choices = [<span class="str">'A'</span>, <span class="str">'B'</span>, <span class="str">'C'</span>]
df[<span class="str">'grade'</span>] = np.select(conditions, choices, default=<span class="str">'F'</span>)`
      },
      {
        title: "Column Operations & Rename",
        desc: "Add, rename, drop, reorder columns",
        code: `<span class="cm"># Add columns</span>
df[<span class="str">'bonus'</span>] = df[<span class="str">'salary'</span>] * <span class="num">0.1</span>
df[<span class="str">'full_name'</span>] = df[<span class="str">'first'</span>] + <span class="str">' '</span> + df[<span class="str">'last'</span>]
df = df.assign(
    bonus=<span class="kw">lambda</span> x: x[<span class="str">'salary'</span>] * <span class="num">0.1</span>,
    net=<span class="kw">lambda</span> x: x[<span class="str">'salary'</span>] + x[<span class="str">'bonus'</span>]  <span class="cm"># chain within assign!</span>
)

<span class="cm"># Rename</span>
df.rename(columns={<span class="str">'old'</span>: <span class="str">'new'</span>, <span class="str">'a'</span>: <span class="str">'b'</span>}, inplace=<span class="kw">True</span>)
df.columns = df.columns.str.lower().str.replace(<span class="str">' '</span>, <span class="str">'_'</span>)  <span class="cm"># clean all names</span>

<span class="cm"># Drop</span>
df.drop(columns=[<span class="str">'col1'</span>, <span class="str">'col2'</span>])
df.drop(index=[<span class="num">0</span>, <span class="num">5</span>, <span class="num">10</span>])

<span class="cm"># Sort</span>
df.sort_values(<span class="str">'age'</span>)
df.sort_values([<span class="str">'dept'</span>, <span class="str">'salary'</span>], ascending=[<span class="kw">True</span>, <span class="kw">False</span>])

<span class="cm"># Reset index</span>
df.reset_index(drop=<span class="kw">True</span>)
df.set_index(<span class="str">'id'</span>)`
      }
    ]
  },

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. AGGREGATION & GROUPBY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  {
    id: "groupby",
    icon: "ğŸ“Š",
    title: "Aggregation & GroupBy",
    level: "intermediate",
    roles: ["de","da"],
    category: "Analysis",
    description: "Summarize data using groupby, agg, pivot tables, and named aggregations.",
    cards: [
      {
        title: "GroupBy Basics â€” All Built-in Agg Functions",
        desc: "Every built-in aggregation function you need to know",
        code: `<span class="cm"># Numeric aggregations</span>
g = df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>]
g.sum()        <span class="cm"># total</span>
g.mean()       <span class="cm"># average</span>
g.median()     <span class="cm"># median</span>
g.std()        <span class="cm"># standard deviation</span>
g.var()        <span class="cm"># variance</span>
g.min()        <span class="cm"># minimum</span>
g.max()        <span class="cm"># maximum</span>
g.count()      <span class="cm"># non-null count</span>
g.size()       <span class="cm"># total count (incl. nulls)</span>
g.nunique()    <span class="cm"># distinct count</span>
g.sem()        <span class="cm"># standard error of mean</span>
g.skew()       <span class="cm"># skewness</span>
g.prod()       <span class="cm"># product</span>
g.first()      <span class="cm"># first value in group</span>
g.last()       <span class="cm"># last value in group</span>
g.quantile(<span class="num">0.25</span>)   <span class="cm"># Q1</span>
g.quantile(<span class="num">0.75</span>)   <span class="cm"># Q3</span>

<span class="cm"># describe per group (all stats at once)</span>
df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>].describe()

<span class="cm"># Multiple groups</span>
df.groupby([<span class="str">'dept'</span>, <span class="str">'city'</span>])[<span class="str">'salary'</span>].mean()

<span class="cm"># Named aggregations (pandas 0.25+)</span>
df.groupby(<span class="str">'dept'</span>).agg(
    avg_salary=(<span class="str">'salary'</span>, <span class="str">'mean'</span>),
    total_salary=(<span class="str">'salary'</span>, <span class="str">'sum'</span>),
    headcount=(<span class="str">'name'</span>, <span class="str">'count'</span>),
    max_age=(<span class="str">'age'</span>, <span class="str">'max'</span>),
    unique_cities=(<span class="str">'city'</span>, <span class="str">'nunique'</span>),
    first_hire=(<span class="str">'hire_date'</span>, <span class="str">'min'</span>)
).reset_index()

<span class="cm"># Custom agg function</span>
df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>].agg(<span class="kw">lambda</span> x: x.quantile(<span class="num">0.75</span>))`
      },
      {
        title: "GroupBy Advanced Patterns",
        desc: "Filter, transform, apply on groups",
        code: `<span class="cm"># filter â€” keep groups matching condition</span>
df.groupby(<span class="str">'dept'</span>).filter(<span class="kw">lambda</span> x: x[<span class="str">'salary'</span>].mean() > <span class="num">60000</span>)

<span class="cm"># transform â€” broadcast group stats back to original rows</span>
df[<span class="str">'dept_avg'</span>] = df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>].transform(<span class="str">'mean'</span>)
df[<span class="str">'salary_rank'</span>] = df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>].rank(ascending=<span class="kw">False</span>)

<span class="cm"># cumulative stats within group</span>
df[<span class="str">'cum_sales'</span>] = df.groupby(<span class="str">'region'</span>)[<span class="str">'sales'</span>].cumsum()
df[<span class="str">'cum_max'</span>] = df.groupby(<span class="str">'region'</span>)[<span class="str">'sales'</span>].cummax()

<span class="cm"># value_counts</span>
df[<span class="str">'dept'</span>].value_counts()
df[<span class="str">'dept'</span>].value_counts(normalize=<span class="kw">True</span>)  <span class="cm"># percentages</span>

<span class="cm"># Pivot table</span>
pt = df.pivot_table(
    values=<span class="str">'salary'</span>,
    index=<span class="str">'dept'</span>,
    columns=<span class="str">'city'</span>,
    aggfunc=<span class="str">'mean'</span>,
    fill_value=<span class="num">0</span>,
    margins=<span class="kw">True</span>   <span class="cm"># add totals</span>
)`
      },
      {
        title: "Crosstab & Cut",
        desc: "Frequency tables and binning",
        code: `<span class="cm"># Crosstab â€” frequency of combinations</span>
pd.crosstab(df[<span class="str">'dept'</span>], df[<span class="str">'city'</span>])
pd.crosstab(df[<span class="str">'dept'</span>], df[<span class="str">'city'</span>], normalize=<span class="str">'index'</span>)  <span class="cm"># row %</span>
pd.crosstab(df[<span class="str">'dept'</span>], df[<span class="str">'city'</span>], values=df[<span class="str">'salary'</span>], aggfunc=<span class="str">'mean'</span>)

<span class="cm"># cut â€” bin continuous into categories</span>
df[<span class="str">'age_band'</span>] = pd.cut(
    df[<span class="str">'age'</span>],
    bins=[<span class="num">0</span>, <span class="num">25</span>, <span class="num">35</span>, <span class="num">45</span>, <span class="num">100</span>],
    labels=[<span class="str">'<25'</span>, <span class="str">'25-35'</span>, <span class="str">'35-45'</span>, <span class="str">'45+'</span>]
)

<span class="cm"># qcut â€” equal-frequency bins</span>
df[<span class="str">'salary_quartile'</span>] = pd.qcut(df[<span class="str">'salary'</span>], q=<span class="num">4</span>, labels=[<span class="str">'Q1'</span>,<span class="str">'Q2'</span>,<span class="str">'Q3'</span>,<span class="str">'Q4'</span>])`
      },
      {
        title: "GroupBy â€” Multiple Aggregation Functions per Column",
        desc: "Different agg functions for different columns simultaneously",
        code: `<span class="cm"># Different aggs per column</span>
df.groupby(<span class="str">'dept'</span>).agg({
    <span class="str">'salary'</span>: [<span class="str">'mean'</span>, <span class="str">'sum'</span>, <span class="str">'std'</span>],
    <span class="str">'age'</span>:    [<span class="str">'min'</span>, <span class="str">'max'</span>],
    <span class="str">'name'</span>:   <span class="str">'count'</span>
})

<span class="cm"># Flatten multi-level column names after agg</span>
grp = df.groupby(<span class="str">'dept'</span>).agg({'salary': ['mean','sum'], 'age': ['min','max']})
grp.columns = [<span class="str">'_'</span>.join(c) <span class="kw">for</span> c <span class="kw">in</span> grp.columns]
grp.reset_index(inplace=<span class="kw">True</span>)

<span class="cm"># Named agg with custom lambda</span>
df.groupby(<span class="str">'dept'</span>).agg(
    avg_sal      = (<span class="str">'salary'</span>, <span class="str">'mean'</span>),
    p90_sal      = (<span class="str">'salary'</span>, <span class="kw">lambda</span> x: x.quantile(<span class="num">0.9</span>)),
    salary_range = (<span class="str">'salary'</span>, <span class="kw">lambda</span> x: x.max() - x.min()),
    active_count = (<span class="str">'status'</span>, <span class="kw">lambda</span> x: (x == <span class="str">'active'</span>).sum()),
    top_earner   = (<span class="str">'name'</span>,   <span class="kw">lambda</span> x: x.iloc[df.loc[x.index, <span class="str">'salary'</span>].argmax()])
).reset_index()

<span class="cm"># Conditional count in groupby</span>
df.groupby(<span class="str">'dept'</span>).agg(
    total      = (<span class="str">'id'</span>, <span class="str">'count'</span>),
    senior_cnt = (<span class="str">'age'</span>, <span class="kw">lambda</span> x: (x >= <span class="num">35</span>).sum()),
    pct_senior = (<span class="str">'age'</span>, <span class="kw">lambda</span> x: (x >= <span class="num">35</span>).mean() * <span class="num">100</span>)
)`
      },
      {
        title: "GroupBy â€” First, Last, Nth, Head, Tail",
        desc: "Get specific rows within each group",
        code: `<span class="cm"># first / last value in each group</span>
df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>].first()
df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>].last()

<span class="cm"># nth â€” get the Nth row per group (0-indexed)</span>
df.groupby(<span class="str">'dept'</span>).nth(<span class="num">0</span>)    <span class="cm"># first row</span>
df.groupby(<span class="str">'dept'</span>).nth(-<span class="num">1</span>)   <span class="cm"># last row</span>
df.groupby(<span class="str">'dept'</span>).nth([<span class="num">0</span>, <span class="num">1</span>]) <span class="cm"># first two rows per group</span>

<span class="cm"># head / tail per group</span>
df.groupby(<span class="str">'dept'</span>).head(<span class="num">3</span>)    <span class="cm"># first 3 rows of each group</span>
df.groupby(<span class="str">'dept'</span>).tail(<span class="num">2</span>)    <span class="cm"># last 2 rows of each group</span>

<span class="cm"># Get row with max/min salary per group (idxmax / idxmin)</span>
idx = df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>].idxmax()
df.loc[idx]   <span class="cm"># top earner row per dept</span>

idx = df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>].idxmin()
df.loc[idx]   <span class="cm"># lowest earner row per dept</span>

<span class="cm"># Concatenate strings within a group</span>
df.groupby(<span class="str">'dept'</span>)[<span class="str">'name'</span>].agg(<span class="str">', '</span>.join)  <span class="cm"># "Alice, Bob, Charlie"</span>`
      },
      {
        title: "GroupBy â€” apply() for Complex Transforms",
        desc: "Full group-level DataFrame operations",
        code: `<span class="cm"># apply passes each group as a DataFrame</span>
<span class="kw">def</span> <span class="fn">top_earners</span>(group, n=<span class="num">2</span>):
    <span class="kw">return</span> group.nlargest(n, <span class="str">'salary'</span>)

df.groupby(<span class="str">'dept'</span>, group_keys=<span class="kw">False</span>).apply(top_earners)
df.groupby(<span class="str">'dept'</span>, group_keys=<span class="kw">False</span>).apply(top_earners, n=<span class="num">3</span>)

<span class="cm"># Normalize salary within each dept (z-score)</span>
<span class="kw">def</span> <span class="fn">zscore</span>(g):
    g[<span class="str">'sal_z'</span>] = (g[<span class="str">'salary'</span>] - g[<span class="str">'salary'</span>].mean()) / g[<span class="str">'salary'</span>].std()
    <span class="kw">return</span> g

df = df.groupby(<span class="str">'dept'</span>, group_keys=<span class="kw">False</span>).apply(zscore)

<span class="cm"># Flag rows where salary > dept median</span>
df[<span class="str">'above_median'</span>] = df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>].transform(
    <span class="kw">lambda</span> x: x > x.median()
)

<span class="cm"># Rolling correlation between two columns within groups</span>
df[<span class="str">'roll_corr'</span>] = (
    df.groupby(<span class="str">'region'</span>)
      .apply(<span class="kw">lambda</span> g: g[<span class="str">'price'</span>].rolling(<span class="num">7</span>).corr(g[<span class="str">'sales'</span>]))
      .reset_index(level=<span class="num">0</span>, drop=<span class="kw">True</span>)
)`
      },
      {
        title: "GroupBy â€” Pivot Table Deep Dive",
        desc: "Pivot table with multiple values, aggfuncs, and margins",
        code: `<span class="cm"># Multi-value pivot table</span>
pd.pivot_table(
    df,
    values=[<span class="str">'salary'</span>, <span class="str">'bonus'</span>],
    index=[<span class="str">'dept'</span>, <span class="str">'city'</span>],
    columns=<span class="str">'year'</span>,
    aggfunc={<span class="str">'salary'</span>: <span class="str">'mean'</span>, <span class="str">'bonus'</span>: <span class="str">'sum'</span>},
    fill_value=<span class="num">0</span>,
    margins=<span class="kw">True</span>,
    margins_name=<span class="str">'Total'</span>
)

<span class="cm"># Pivot table â†’ reset â†’ clean column names</span>
pt = df.pivot_table(values=<span class="str">'sales'</span>, index=<span class="str">'region'</span>, columns=<span class="str">'category'</span>, aggfunc=<span class="str">'sum'</span>, fill_value=<span class="num">0</span>)
pt.columns.name = <span class="kw">None</span>
pt = pt.reset_index()

<span class="cm"># Use observed=True with category dtype (avoids empty combos)</span>
df[<span class="str">'dept'</span>] = df[<span class="str">'dept'</span>].astype(<span class="str">'category'</span>)
df.groupby(<span class="str">'dept'</span>, observed=<span class="kw">True</span>)[<span class="str">'salary'</span>].mean()

<span class="cm"># sort_values on grouped result</span>
df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>].mean().sort_values(ascending=<span class="kw">False</span>).head(<span class="num">5</span>)`
      }
    ]
  },

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. JOINS & MERGES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  {
    id: "joins",
    icon: "ğŸ”—",
    title: "Joins & Merges",
    level: "intermediate",
    roles: ["de","da"],
    category: "Analysis",
    description: "Combine DataFrames with merge, join, and concat â€” the SQL joins equivalent.",
    cards: [
      {
        title: "merge â€” SQL-style joins",
        desc: "The pandas equivalent of SQL JOIN",
        code: `<span class="cm"># Inner join (default)</span>
merged = pd.merge(df1, df2, on=<span class="str">'id'</span>)

<span class="cm"># All join types</span>
pd.merge(df1, df2, on=<span class="str">'id'</span>, how=<span class="str">'inner'</span>)  <span class="cm"># matching rows only</span>
pd.merge(df1, df2, on=<span class="str">'id'</span>, how=<span class="str">'left'</span>)   <span class="cm"># all left, match right</span>
pd.merge(df1, df2, on=<span class="str">'id'</span>, how=<span class="str">'right'</span>)  <span class="cm"># all right, match left</span>
pd.merge(df1, df2, on=<span class="str">'id'</span>, how=<span class="str">'outer'</span>)  <span class="cm"># all rows from both</span>

<span class="cm"># Different key column names</span>
pd.merge(orders, customers, left_on=<span class="str">'cust_id'</span>, right_on=<span class="str">'id'</span>)

<span class="cm"># Multiple keys</span>
pd.merge(df1, df2, on=[<span class="str">'year'</span>, <span class="str">'dept'</span>])

<span class="cm"># Handle duplicate column names</span>
pd.merge(df1, df2, on=<span class="str">'id'</span>, suffixes=(<span class="str">'_left'</span>, <span class="str">'_right'</span>))

<span class="cm"># Anti-join (find non-matching)</span>
merged = pd.merge(df1, df2, on=<span class="str">'id'</span>, how=<span class="str">'left'</span>, indicator=<span class="kw">True</span>)
anti = merged[merged[<span class="str">'_merge'</span>] == <span class="str">'left_only'</span>].drop(<span class="str">'_merge'</span>, axis=<span class="num">1</span>)`
      },
      {
        title: "concat & combine",
        desc: "Stack DataFrames vertically or horizontally",
        code: `<span class="cm"># Vertical stack (union)</span>
pd.concat([df1, df2, df3], ignore_index=<span class="kw">True</span>)
pd.concat([df1, df2], ignore_index=<span class="kw">True</span>, sort=<span class="kw">False</span>)

<span class="cm"># Horizontal stack (side by side)</span>
pd.concat([df1, df2], axis=<span class="num">1</span>)

<span class="cm"># With source keys</span>
pd.concat({<span class="str">'jan'</span>: df_jan, <span class="str">'feb'</span>: df_feb}, names=[<span class="str">'month'</span>, <span class="str">'idx'</span>])

<span class="cm"># join â€” index-based merge</span>
df1.join(df2, how=<span class="str">'left'</span>)
df1.join(df2, on=<span class="str">'key_col'</span>)

<span class="cm"># combine_first â€” fill nulls from another df</span>
df1.combine_first(df2)  <span class="cm"># df2 fills where df1 is null</span>

<span class="cm"># update â€” update values in-place</span>
df1.update(df2)  <span class="cm"># non-NA values of df2 overwrite df1</span>`
      },
      {
        title: "Anti-Join, Semi-Join, Self-Join",
        desc: "SQL patterns not natively in pandas but easy to build",
        code: `<span class="cm"># â”€â”€ ANTI JOIN â€” rows in left NOT in right â”€â”€</span>
merged = pd.merge(orders, cancelled, on=<span class="str">'order_id'</span>, how=<span class="str">'left'</span>, indicator=<span class="kw">True</span>)
anti = merged[merged[<span class="str">'_merge'</span>] == <span class="str">'left_only'</span>].drop(columns=<span class="str">'_merge'</span>)

<span class="cm"># â”€â”€ SEMI JOIN â€” rows in left that EXIST in right (no right cols) â”€â”€</span>
semi = df1[df1[<span class="str">'id'</span>].isin(df2[<span class="str">'id'</span>])]

<span class="cm"># â”€â”€ SELF JOIN â€” join a table to itself â”€â”€</span>
<span class="cm"># E.g., find employee-manager pairs</span>
result = pd.merge(
    emp, emp[[<span class="str">'emp_id'</span>, <span class="str">'name'</span>]].rename(columns={<span class="str">'emp_id'</span>: <span class="str">'mgr_id'</span>, <span class="str">'name'</span>: <span class="str">'mgr_name'</span>}),
    on=<span class="str">'mgr_id'</span>, how=<span class="str">'left'</span>
)

<span class="cm"># â”€â”€ CROSS JOIN â€” every row Ã— every row â”€â”€</span>
df1[<span class="str">'_key'</span>] = <span class="num">1</span>
df2[<span class="str">'_key'</span>] = <span class="num">1</span>
cross = pd.merge(df1, df2, on=<span class="str">'_key'</span>).drop(columns=<span class="str">'_key'</span>)
<span class="cm"># Pandas 1.2+: cleaner cross join</span>
cross = pd.merge(df1, df2, how=<span class="str">'cross'</span>)`
      },
      {
        title: "Merge Validation & Diagnostics",
        desc: "Catch bad joins before they corrupt data",
        code: `<span class="cm"># validate â€” enforce join cardinality</span>
pd.merge(df1, df2, on=<span class="str">'id'</span>, validate=<span class="str">'1:1'</span>)   <span class="cm"># raises if not one-to-one</span>
pd.merge(df1, df2, on=<span class="str">'id'</span>, validate=<span class="str">'1:m'</span>)   <span class="cm"># one left to many right</span>
pd.merge(df1, df2, on=<span class="str">'id'</span>, validate=<span class="str">'m:1'</span>)   <span class="cm"># many left to one right</span>
pd.merge(df1, df2, on=<span class="str">'id'</span>, validate=<span class="str">'m:m'</span>)   <span class="cm"># many to many (default)</span>

<span class="cm"># indicator â€” see where each row came from</span>
merged = pd.merge(df1, df2, on=<span class="str">'id'</span>, how=<span class="str">'outer'</span>, indicator=<span class="kw">True</span>)
merged[<span class="str">'_merge'</span>].value_counts()
<span class="cm"># 'both'        â†’ matched rows</span>
<span class="cm"># 'left_only'   â†’ in df1 only</span>
<span class="cm"># 'right_only'  â†’ in df2 only</span>

<span class="cm"># Check for row count explosion (fan-out)</span>
before = len(df1)
result = pd.merge(df1, df2, on=<span class="str">'id'</span>)
after  = len(result)
<span class="kw">if</span> after > before:
    <span class="fn">print</span>(<span class="str">f"âš ï¸  Fan-out! {before} â†’ {after} rows. Check for duplicates in df2['id']"</span>)

<span class="cm"># Find duplicate keys before merging</span>
<span class="fn">print</span>(df2[df2[<span class="str">'id'</span>].duplicated(keep=<span class="kw">False</span>)])  <span class="cm"># show dupe keys in right table</span>`
      },
      {
        title: "Merge on Multiple Keys & Inequality Joins",
        desc: "Complex join conditions",
        code: `<span class="cm"># Merge on multiple columns</span>
pd.merge(sales, targets, on=[<span class="str">'year'</span>, <span class="str">'quarter'</span>, <span class="str">'region'</span>])

<span class="cm"># Merge on keys with different names</span>
pd.merge(
    orders.rename(columns={<span class="str">'cust_id'</span>: <span class="str">'id'</span>}),
    customers,
    on=<span class="str">'id'</span>
)
<span class="cm"># Or use left_on / right_on:</span>
pd.merge(orders, customers, left_on=<span class="str">'cust_id'</span>, right_on=<span class="str">'customer_id'</span>)

<span class="cm"># Asof merge â€” nearest key join (like SQL range join)</span>
<span class="cm"># Match each trade to the most recent quote</span>
pd.merge_asof(
    trades.sort_values(<span class="str">'time'</span>),
    quotes.sort_values(<span class="str">'time'</span>),
    on=<span class="str">'time'</span>,
    by=<span class="str">'ticker'</span>,          <span class="cm"># must also match ticker</span>
    direction=<span class="str">'backward'</span>  <span class="cm"># nearest quote â‰¤ trade time</span>
)

<span class="cm"># merge_ordered â€” ordered merge with fill</span>
pd.merge_ordered(df1, df2, on=<span class="str">'date'</span>, fill_method=<span class="str">'ffill'</span>)

<span class="cm"># Join on index</span>
df1.set_index(<span class="str">'id'</span>).join(df2.set_index(<span class="str">'id'</span>), how=<span class="str">'inner'</span>, lsuffix=<span class="str">'_l'</span>, rsuffix=<span class="str">'_r'</span>)`
      }
    ]
  },

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8. DATETIME â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  {
    id: "datetime",
    icon: "ğŸ“…",
    title: "DateTime Operations",
    level: "intermediate",
    roles: ["de","da"],
    category: "Analysis",
    description: "Parse, manipulate, resample, and extract from date/time columns.",
    cards: [
      {
        title: "Parsing & .dt accessor",
        desc: "Convert strings to datetime and extract components",
        code: `<span class="cm"># Parse</span>
df[<span class="str">'date'</span>] = pd.to_datetime(df[<span class="str">'date'</span>])
df[<span class="str">'date'</span>] = pd.to_datetime(df[<span class="str">'date'</span>], format=<span class="str">'%Y-%m-%d'</span>)
df[<span class="str">'date'</span>] = pd.to_datetime(df[<span class="str">'date'</span>], utc=<span class="kw">True</span>)

<span class="cm"># .dt accessor</span>
df[<span class="str">'year'</span>] = df[<span class="str">'date'</span>].dt.year
df[<span class="str">'month'</span>] = df[<span class="str">'date'</span>].dt.month
df[<span class="str">'day'</span>] = df[<span class="str">'date'</span>].dt.day
df[<span class="str">'hour'</span>] = df[<span class="str">'date'</span>].dt.hour
df[<span class="str">'weekday'</span>] = df[<span class="str">'date'</span>].dt.dayofweek  <span class="cm"># 0=Mon</span>
df[<span class="str">'quarter'</span>] = df[<span class="str">'date'</span>].dt.quarter
df[<span class="str">'is_month_end'</span>] = df[<span class="str">'date'</span>].dt.is_month_end
df[<span class="str">'week'</span>] = df[<span class="str">'date'</span>].dt.isocalendar().week
df[<span class="str">'day_name'</span>] = df[<span class="str">'date'</span>].dt.day_name()
df[<span class="str">'str_date'</span>] = df[<span class="str">'date'</span>].dt.strftime(<span class="str">'%B %Y'</span>)`
      },
      {
        title: "Time Series & Resampling",
        desc: "Resample, rolling windows, date ranges",
        code: `<span class="cm"># Date ranges</span>
pd.date_range(<span class="str">'2024-01-01'</span>, periods=<span class="num">12</span>, freq=<span class="str">'ME'</span>)  <span class="cm"># monthly</span>
pd.date_range(<span class="str">'2024-01-01'</span>, <span class="str">'2024-12-31'</span>, freq=<span class="str">'D'</span>)  <span class="cm"># daily</span>

<span class="cm"># Set datetime index for time series</span>
df = df.set_index(<span class="str">'date'</span>)

<span class="cm"># Resample â€” aggregate by time period</span>
df.resample(<span class="str">'ME'</span>)[<span class="str">'sales'</span>].sum()     <span class="cm"># monthly sum</span>
df.resample(<span class="str">'W'</span>)[<span class="str">'sales'</span>].mean()      <span class="cm"># weekly mean</span>
df.resample(<span class="str">'QE'</span>)[<span class="str">'sales'</span>].agg([<span class="str">'sum'</span>, <span class="str">'mean'</span>])

<span class="cm"># Rolling windows</span>
df[<span class="str">'7d_avg'</span>] = df[<span class="str">'sales'</span>].rolling(window=<span class="num">7</span>).mean()
df[<span class="str">'30d_sum'</span>] = df[<span class="str">'sales'</span>].rolling(window=<span class="num">30</span>).sum()
df[<span class="str">'7d_std'</span>] = df[<span class="str">'sales'</span>].rolling(window=<span class="num">7</span>).std()

<span class="cm"># Shift â€” lag/lead</span>
df[<span class="str">'prev_sales'</span>] = df[<span class="str">'sales'</span>].shift(<span class="num">1</span>)     <span class="cm"># lag 1</span>
df[<span class="str">'next_sales'</span>] = df[<span class="str">'sales'</span>].shift(-<span class="num">1</span>)    <span class="cm"># lead 1</span>
df[<span class="str">'mom_growth'</span>] = df[<span class="str">'sales'</span>].pct_change()  <span class="cm"># % change</span>

<span class="cm"># Timezone handling</span>
df[<span class="str">'date'</span>].dt.tz_localize(<span class="str">'UTC'</span>)
df[<span class="str">'date'</span>].dt.tz_convert(<span class="str">'Asia/Kolkata'</span>)`
      }
    ]
  },

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 9. RESHAPE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  {
    id: "reshape",
    icon: "ğŸ”„",
    title: "Reshaping Data",
    level: "intermediate",
    roles: ["de","da"],
    category: "Analysis",
    description: "Pivot, melt, stack, unstack, and explode to reshape DataFrames.",
    cards: [
      {
        title: "Pivot, Melt, Stack, Unstack",
        desc: "Wide â†” Long transformations",
        code: `<span class="cm"># pivot â€” long to wide</span>
df.pivot(index=<span class="str">'date'</span>, columns=<span class="str">'product'</span>, values=<span class="str">'sales'</span>)

<span class="cm"># melt â€” wide to long (unpivot)</span>
df.melt(
    id_vars=[<span class="str">'id'</span>, <span class="str">'name'</span>],
    value_vars=[<span class="str">'jan_sales'</span>, <span class="str">'feb_sales'</span>, <span class="str">'mar_sales'</span>],
    var_name=<span class="str">'month'</span>,
    value_name=<span class="str">'sales'</span>
)

<span class="cm"># stack / unstack â€” with MultiIndex</span>
df.stack()      <span class="cm"># cols â†’ row level</span>
df.unstack()    <span class="cm"># row level â†’ cols</span>

<span class="cm"># explode â€” list values to multiple rows</span>
df[<span class="str">'tags'</span>] = [<span class="str">'[a, b]'</span>, <span class="str">'[c]'</span>]
df[<span class="str">'tags'</span>] = df[<span class="str">'tags'</span>].str.split(<span class="str">','</span>)
df.explode(<span class="str">'tags'</span>)  <span class="cm"># one row per tag

# Wide to long: pd.wide_to_long</span>
pd.wide_to_long(df, stubnames=[<span class="str">'sales'</span>, <span class="str">'cost'</span>], i=<span class="str">'id'</span>, j=<span class="str">'year'</span>)`
      }
    ]
  },

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 10. WINDOW FUNCTIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  {
    id: "window",
    icon: "ğŸªŸ",
    title: "Window Functions",
    level: "advanced",
    roles: ["de","da"],
    category: "Advanced",
    description: "Rank, cumulative, rolling, and expanding window calculations.",
    cards: [
      {
        title: "Ranking & Cumulative",
        desc: "SQL RANK, DENSE_RANK, ROW_NUMBER equivalents",
        code: `<span class="cm"># rank â€” multiple methods</span>
df[<span class="str">'rank'</span>] = df[<span class="str">'salary'</span>].rank()
df[<span class="str">'rank_min'</span>] = df[<span class="str">'salary'</span>].rank(method=<span class="str">'min'</span>)     <span class="cm"># like RANK()</span>
df[<span class="str">'rank_dense'</span>] = df[<span class="str">'salary'</span>].rank(method=<span class="str">'dense'</span>)   <span class="cm"># like DENSE_RANK()</span>
df[<span class="str">'row_num'</span>] = df[<span class="str">'salary'</span>].rank(method=<span class="str">'first'</span>)    <span class="cm"># like ROW_NUMBER()</span>

<span class="cm"># Rank within group (window function)</span>
df[<span class="str">'dept_rank'</span>] = df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>].rank(
    method=<span class="str">'dense'</span>, ascending=<span class="kw">False</span>
)

<span class="cm"># Top N per group</span>
top2 = df[df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>].rank(method=<span class="str">'dense'</span>, ascending=<span class="kw">False</span>) <= <span class="num">2</span>]

<span class="cm"># Cumulative functions</span>
df[<span class="str">'cum_sum'</span>] = df[<span class="str">'sales'</span>].cumsum()
df[<span class="str">'cum_max'</span>] = df[<span class="str">'sales'</span>].cummax()
df[<span class="str">'cum_prod'</span>] = df[<span class="str">'sales'</span>].cumprod()
df[<span class="str">'cum_pct'</span>] = df[<span class="str">'sales'</span>].cumsum() / df[<span class="str">'sales'</span>].sum() * <span class="num">100</span>  <span class="cm"># Pareto</span>`
      },
      {
        title: "Rolling & Expanding Windows",
        desc: "Moving averages and expanding aggregates",
        code: `<span class="cm"># Rolling â€” fixed window size</span>
df[<span class="str">'sales'</span>].rolling(<span class="num">7</span>).mean()     <span class="cm"># 7-day moving avg</span>
df[<span class="str">'sales'</span>].rolling(<span class="num">7</span>).sum()
df[<span class="str">'sales'</span>].rolling(<span class="num">7</span>, min_periods=<span class="num">1</span>).mean()  <span class="cm"># no NaN at start</span>
df[<span class="str">'sales'</span>].rolling(<span class="num">7</span>, center=<span class="kw">True</span>).mean()    <span class="cm"># centered window</span>

<span class="cm"># Expanding â€” all rows up to current</span>
df[<span class="str">'sales'</span>].expanding().mean()   <span class="cm"># running avg</span>
df[<span class="str">'sales'</span>].expanding().max()    <span class="cm"># running max</span>

<span class="cm"># Exponential weighted moving avg</span>
df[<span class="str">'ewm'</span>] = df[<span class="str">'sales'</span>].ewm(span=<span class="num">7</span>, adjust=<span class="kw">False</span>).mean()

<span class="cm"># Rolling within groups</span>
df[<span class="str">'rolling_avg'</span>] = (
    df.sort_values(<span class="str">'date'</span>)
      .groupby(<span class="str">'region'</span>)[<span class="str">'sales'</span>]
      .transform(<span class="kw">lambda</span> x: x.rolling(<span class="num">7</span>, min_periods=<span class="num">1</span>).mean())
)`
      },
      {
        title: "SQL Window Functions â€” Full Equivalents",
        desc: "LAG, LEAD, FIRST_VALUE, LAST_VALUE, NTILE in pandas",
        code: `<span class="cm"># LAG(sales, 1) â€” previous row value</span>
df[<span class="str">'lag_1'</span>] = df[<span class="str">'sales'</span>].shift(<span class="num">1</span>)
df[<span class="str">'lag_1_by_grp'</span>] = df.groupby(<span class="str">'region'</span>)[<span class="str">'sales'</span>].shift(<span class="num">1</span>)

<span class="cm"># LEAD(sales, 1) â€” next row value</span>
df[<span class="str">'lead_1'</span>] = df[<span class="str">'sales'</span>].shift(-<span class="num">1</span>)

<span class="cm"># FIRST_VALUE() OVER (PARTITION BY dept ORDER BY date)</span>
df[<span class="str">'first_sale'</span>] = (
    df.sort_values(<span class="str">'date'</span>)
      .groupby(<span class="str">'dept'</span>)[<span class="str">'sales'</span>]
      .transform(<span class="str">'first'</span>)
)

<span class="cm"># LAST_VALUE()</span>
df[<span class="str">'last_sale'</span>] = (
    df.sort_values(<span class="str">'date'</span>)
      .groupby(<span class="str">'dept'</span>)[<span class="str">'sales'</span>]
      .transform(<span class="str">'last'</span>)
)

<span class="cm"># NTILE(4) â€” divide into N equal buckets</span>
df[<span class="str">'ntile_4'</span>] = pd.qcut(df[<span class="str">'sales'</span>].rank(method=<span class="str">'first'</span>),
                         q=<span class="num">4</span>, labels=[<span class="num">1</span>, <span class="num">2</span>, <span class="num">3</span>, <span class="num">4</span>]).astype(int)

<span class="cm"># PERCENT_RANK()</span>
df[<span class="str">'pct_rank'</span>] = df[<span class="str">'sales'</span>].rank(pct=<span class="kw">True</span>)

<span class="cm"># SUM() OVER () â€” running total (no partition)</span>
df[<span class="str">'running_total'</span>] = df[<span class="str">'sales'</span>].cumsum()

<span class="cm"># SUM() OVER (PARTITION BY region)</span>
df[<span class="str">'region_total'</span>] = df.groupby(<span class="str">'region'</span>)[<span class="str">'sales'</span>].transform(<span class="str">'sum'</span>)`
      },
      {
        title: "Window â€” Pct Change, Diff, Z-Score",
        desc: "Relative change and normalization over time",
        code: `<span class="cm"># pct_change â€” period-over-period % growth</span>
df[<span class="str">'mom_growth'</span>] = df[<span class="str">'sales'</span>].pct_change()          <span class="cm"># month-over-month</span>
df[<span class="str">'yoy_growth'</span>] = df[<span class="str">'sales'</span>].pct_change(periods=<span class="num">12</span>)  <span class="cm"># year-over-year</span>

<span class="cm"># diff â€” absolute change</span>
df[<span class="str">'delta'</span>] = df[<span class="str">'sales'</span>].diff(<span class="num">1</span>)
df[<span class="str">'delta_7d'</span>] = df[<span class="str">'sales'</span>].diff(<span class="num">7</span>)

<span class="cm"># Z-score across all rows</span>
df[<span class="str">'z'</span>] = (df[<span class="str">'sales'</span>] - df[<span class="str">'sales'</span>].mean()) / df[<span class="str">'sales'</span>].std()

<span class="cm"># Z-score within group (groupby + transform)</span>
df[<span class="str">'z_within_dept'</span>] = df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>].transform(
    <span class="kw">lambda</span> x: (x - x.mean()) / x.std()
)

<span class="cm"># Rolling z-score</span>
roll = df[<span class="str">'sales'</span>].rolling(<span class="num">30</span>)
df[<span class="str">'roll_z'</span>] = (df[<span class="str">'sales'</span>] - roll.mean()) / roll.std()

<span class="cm"># Min-max normalization within group</span>
df[<span class="str">'norm'</span>] = df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>].transform(
    <span class="kw">lambda</span> x: (x - x.min()) / (x.max() - x.min())
)`
      },
      {
        title: "Window â€” Rolling Custom Functions & Multiple Outputs",
        desc: "Advanced rolling/expanding with custom logic",
        code: `<span class="cm"># Rolling with custom function (slower but flexible)</span>
df[<span class="str">'roll_iqr'</span>] = df[<span class="str">'sales'</span>].rolling(<span class="num">30</span>).apply(
    <span class="kw">lambda</span> x: np.percentile(x, <span class="num">75</span>) - np.percentile(x, <span class="num">25</span>), raw=<span class="kw">True</span>
)

<span class="cm"># Rolling coefficient of variation</span>
df[<span class="str">'roll_cv'</span>] = df[<span class="str">'sales'</span>].rolling(<span class="num">30</span>).std() / df[<span class="str">'sales'</span>].rolling(<span class="num">30</span>).mean()

<span class="cm"># Multiple rolling stats at once</span>
roll = df[<span class="str">'sales'</span>].rolling(<span class="num">7</span>)
stats = pd.concat([
    roll.mean().rename(<span class="str">'r7_mean'</span>),
    roll.std().rename(<span class="str">'r7_std'</span>),
    roll.min().rename(<span class="str">'r7_min'</span>),
    roll.max().rename(<span class="str">'r7_max'</span>),
], axis=<span class="num">1</span>)
df = pd.concat([df, stats], axis=<span class="num">1</span>)

<span class="cm"># Bollinger bands</span>
window = <span class="num">20</span>
df[<span class="str">'mid'</span>]   = df[<span class="str">'close'</span>].rolling(window).mean()
df[<span class="str">'upper'</span>] = df[<span class="str">'mid'</span>] + <span class="num">2</span> * df[<span class="str">'close'</span>].rolling(window).std()
df[<span class="str">'lower'</span>] = df[<span class="str">'mid'</span>] - <span class="num">2</span> * df[<span class="str">'close'</span>].rolling(window).std()

<span class="cm"># Detect outliers with rolling window</span>
roll = df[<span class="str">'sales'</span>].rolling(<span class="num">30</span>, center=<span class="kw">True</span>)
df[<span class="str">'is_outlier'</span>] = (df[<span class="str">'sales'</span>] - roll.mean()).abs() > <span class="num">3</span> * roll.std()`
      }
    ]
  },

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 10.5 AGGREGATION RECIPES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  {
    id: "aggrec",
    icon: "ğŸ§®",
    title: "Aggregation Recipes",
    level: "advanced",
    roles: ["de","da"],
    category: "Advanced",
    description: "Real-world aggregation patterns: conditional counts, weighted avg, first/last, string concat, and business metrics.",
    cards: [
      {
        title: "Conditional & Weighted Aggregations",
        desc: "Count conditionally, weighted average, mode",
        code: `<span class="cm"># Conditional count â€” how many salaries > 60k per dept</span>
df.groupby(<span class="str">'dept'</span>).agg(
    high_earners=(<span class="str">'salary'</span>, <span class="kw">lambda</span> x: (x > <span class="num">60000</span>).sum()),
    pct_high=(<span class="str">'salary'</span>, <span class="kw">lambda</span> x: (x > <span class="num">60000</span>).mean() * <span class="num">100</span>)
)

<span class="cm"># Weighted average (revenue-weighted avg price)</span>
df.groupby(<span class="str">'product'</span>).apply(
    <span class="kw">lambda</span> g: np.average(g[<span class="str">'price'</span>], weights=g[<span class="str">'qty'</span>])
).rename(<span class="str">'weighted_avg_price'</span>)

<span class="cm"># Mode per group</span>
df.groupby(<span class="str">'dept'</span>)[<span class="str">'city'</span>].agg(
    <span class="kw">lambda</span> x: x.mode().iloc[<span class="num">0</span>] <span class="kw">if</span> len(x.mode()) > <span class="num">0</span> <span class="kw">else</span> np.nan
)

<span class="cm"># Median absolute deviation (MAD)</span>
df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>].agg(
    <span class="kw">lambda</span> x: (x - x.median()).abs().median()
)

<span class="cm"># Multiple percentiles per group</span>
df.groupby(<span class="str">'dept'</span>)[<span class="str">'salary'</span>].agg(
    p25=<span class="kw">lambda</span> x: x.quantile(<span class="num">0.25</span>),
    p50=<span class="kw">lambda</span> x: x.quantile(<span class="num">0.50</span>),
    p75=<span class="kw">lambda</span> x: x.quantile(<span class="num">0.75</span>),
    p90=<span class="kw">lambda</span> x: x.quantile(<span class="num">0.90</span>),
    p99=<span class="kw">lambda</span> x: x.quantile(<span class="num">0.99</span>)
)`
      },
      {
        title: "String & List Aggregations",
        desc: "Collect values within a group as strings or lists",
        code: `<span class="cm"># Concatenate strings per group</span>
df.groupby(<span class="str">'dept'</span>)[<span class="str">'name'</span>].agg(<span class="str">', '</span>.join)

<span class="cm"># Collect as list</span>
df.groupby(<span class="str">'dept'</span>)[<span class="str">'name'</span>].agg(list)
df.groupby(<span class="str">'dept'</span>)[<span class="str">'name'</span>].agg(set)     <span class="cm"># unique values</span>
df.groupby(<span class="str">'dept'</span>)[<span class="str">'name'</span>].apply(list).reset_index()

<span class="cm"># Collect unique sorted values as string</span>
df.groupby(<span class="str">'order_id'</span>)[<span class="str">'tag'</span>].agg(<span class="kw">lambda</span> x: <span class="str">'|'</span>.join(sorted(x.unique())))

<span class="cm"># Most frequent value + its count</span>
df.groupby(<span class="str">'dept'</span>)[<span class="str">'city'</span>].agg([
    (<span class="str">'top_city'</span>, <span class="kw">lambda</span> x: x.mode().iloc[<span class="num">0</span>]),
    (<span class="str">'top_city_cnt'</span>, <span class="kw">lambda</span> x: x.value_counts().iloc[<span class="num">0</span>])
])`
      },
      {
        title: "Business Metrics â€” MoM, YoY, Running Totals",
        desc: "Common DA/reporting aggregation patterns",
        code: `<span class="cm"># Month-over-month revenue change</span>
monthly = df.resample(<span class="str">'ME'</span>, on=<span class="str">'order_date'</span>)[<span class="str">'revenue'</span>].sum().reset_index()
monthly[<span class="str">'mom_change'</span>]  = monthly[<span class="str">'revenue'</span>].diff()
monthly[<span class="str">'mom_pct'</span>]    = monthly[<span class="str">'revenue'</span>].pct_change() * <span class="num">100</span>

<span class="cm"># Year-over-year using groupby</span>
df[<span class="str">'year'</span>]  = df[<span class="str">'date'</span>].dt.year
df[<span class="str">'month'</span>] = df[<span class="str">'date'</span>].dt.month
yoy = df.groupby([<span class="str">'month'</span>, <span class="str">'year'</span>])[<span class="str">'sales'</span>].sum().unstack(<span class="str">'year'</span>)

<span class="cm"># Cumulative revenue (running total) by region</span>
df = df.sort_values([<span class="str">'region'</span>, <span class="str">'date'</span>])
df[<span class="str">'cum_rev'</span>] = df.groupby(<span class="str">'region'</span>)[<span class="str">'revenue'</span>].cumsum()

<span class="cm"># % of group total (contribution)</span>
df[<span class="str">'region_total'</span>] = df.groupby(<span class="str">'region'</span>)[<span class="str">'sales'</span>].transform(<span class="str">'sum'</span>)
df[<span class="str">'pct_of_region'</span>] = df[<span class="str">'sales'</span>] / df[<span class="str">'region_total'</span>] * <span class="num">100</span>

<span class="cm"># ABC analysis (Pareto 80/20)</span>
prod = df.groupby(<span class="str">'product'</span>)[<span class="str">'revenue'</span>].sum().sort_values(ascending=<span class="kw">False</span>).reset_index()
prod[<span class="str">'cum_pct'</span>] = prod[<span class="str">'revenue'</span>].cumsum() / prod[<span class="str">'revenue'</span>].sum() * <span class="num">100</span>
prod[<span class="str">'abc'</span>] = pd.cut(prod[<span class="str">'cum_pct'</span>], bins=[<span class="num">0</span>,<span class="num">80</span>,<span class="num">95</span>,<span class="num">100</span>], labels=[<span class="str">'A'</span>,<span class="str">'B'</span>,<span class="str">'C'</span>])`
      },
      {
        title: "Cohort & Retention Analysis",
        desc: "User retention / cohort tables with groupby",
        code: `<span class="cm"># Step 1 â€” assign cohort (first purchase month)</span>
df[<span class="str">'order_date'</span>] = pd.to_datetime(df[<span class="str">'order_date'</span>])
df[<span class="str">'cohort'</span>] = (
    df.groupby(<span class="str">'user_id'</span>)[<span class="str">'order_date'</span>]
      .transform(<span class="str">'min'</span>)
      .dt.to_period(<span class="str">'M'</span>)
)
df[<span class="str">'order_period'</span>] = df[<span class="str">'order_date'</span>].dt.to_period(<span class="str">'M'</span>)

<span class="cm"># Step 2 â€” period index (months since cohort)</span>
df[<span class="str">'period_idx'</span>] = (df[<span class="str">'order_period'</span>] - df[<span class="str">'cohort'</span>]).apply(attrgetter(<span class="str">'n'</span>))

<span class="cm"># Step 3 â€” cohort size</span>
cohort_size = df.groupby(<span class="str">'cohort'</span>)[<span class="str">'user_id'</span>].nunique()

<span class="cm"># Step 4 â€” retention table</span>
retention = (
    df.groupby([<span class="str">'cohort'</span>, <span class="str">'period_idx'</span>])[<span class="str">'user_id'</span>]
      .nunique()
      .unstack()
      .div(cohort_size, axis=<span class="num">0</span>)
      .round(<span class="num">2</span>)
)
<span class="fn">print</span>(retention)`
      }
    ]
  },
  {
    id: "performance",
    icon: "âš¡",
    title: "Performance Optimization",
    level: "advanced",
    roles: ["de"],
    category: "Advanced",
    description: "Chunk processing, vectorization, memory optimization, and eval/query tricks.",
    cards: [
      {
        title: "Vectorization vs. Apply",
        desc: "Write fast pandas â€” avoid loops",
        code: `<span class="cm"># SLOW â€” never do this</span>
<span class="kw">for</span> i, row <span class="kw">in</span> df.iterrows():
    df.at[i, <span class="str">'tax'</span>] = row[<span class="str">'income'</span>] * <span class="num">0.3</span>

<span class="cm"># FAST â€” vectorized (100x faster)</span>
df[<span class="str">'tax'</span>] = df[<span class="str">'income'</span>] * <span class="num">0.3</span>

<span class="cm"># Speed hierarchy (slowest â†’ fastest)</span>
<span class="cm"># iterrows() < apply() < numpy vectorized < eval()</span>

<span class="cm"># Use eval() for large DataFrames</span>
df.eval(<span class="str">'total = price * quantity - discount'</span>, inplace=<span class="kw">True</span>)
result = df.eval(<span class="str">'(price - cost) / price * 100'</span>)

<span class="cm"># Use numpy directly when possible</span>
df[<span class="str">'abs_val'</span>] = np.abs(df[<span class="str">'col'</span>])
df[<span class="str">'log_val'</span>] = np.log1p(df[<span class="str">'col'</span>])

<span class="cm"># Avoid chained indexing</span>
df[<span class="str">'col'</span>][df[<span class="str">'age'</span>] > <span class="num">30</span>] = <span class="num">0</span>  <span class="cm"># BAD â€” SettingWithCopyWarning</span>
df.loc[df[<span class="str">'age'</span>] > <span class="num">30</span>, <span class="str">'col'</span>] = <span class="num">0</span>  <span class="cm"># GOOD</span>`
      },
      {
        title: "Chunked Processing (Large Files)",
        desc: "Process files larger than RAM",
        code: `<span class="cm"># Process CSV in chunks</span>
results = []
<span class="kw">for</span> chunk <span class="kw">in</span> pd.read_csv(<span class="str">'huge.csv'</span>, chunksize=<span class="num">100_000</span>):
    chunk = chunk[chunk[<span class="str">'status'</span>] == <span class="str">'active'</span>]
    chunk[<span class="str">'tax'</span>] = chunk[<span class="str">'income'</span>] * <span class="num">0.3</span>
    results.append(chunk.groupby(<span class="str">'region'</span>)[<span class="str">'tax'</span>].sum())

final = pd.concat(results).groupby(level=<span class="num">0</span>).sum()

<span class="cm"># Efficient copy vs view</span>
df2 = df.copy()         <span class="cm"># deep copy (safe to modify)</span>
df2 = df[<span class="str">'col'</span>].copy()  <span class="cm"># copy series</span>

<span class="cm"># Memory usage</span>
df.memory_usage(deep=<span class="kw">True</span>).sum() / <span class="num">1e6</span>  <span class="cm"># MB

# Use categories to save memory</span>
df[<span class="str">'country'</span>] = df[<span class="str">'country'</span>].astype(<span class="str">'category'</span>)  <span class="cm"># 10x smaller

# Parquet is far more efficient than CSV for DE</span>
df.to_parquet(<span class="str">'data.parquet'</span>)       <span class="cm"># 5-10x smaller + faster read</span>
df.read_parquet(<span class="str">'data.parquet'</span>, columns=[<span class="str">'col1'</span>])  <span class="cm"># column pruning</span>`
      }
    ]
  },

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 12. ADVANCED PATTERNS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  {
    id: "advanced",
    icon: "ğŸ§ ",
    title: "Advanced Patterns",
    level: "advanced",
    roles: ["de","da"],
    category: "Advanced",
    description: "MultiIndex, pipes, method chaining, custom accessors, and real-world DE/DA patterns.",
    cards: [
      {
        title: "MultiIndex",
        desc: "Hierarchical row and column indexing",
        code: `<span class="cm"># Create MultiIndex</span>
arrays = [[<span class="str">'A'</span>, <span class="str">'A'</span>, <span class="str">'B'</span>, <span class="str">'B'</span>], [<span class="str">'x'</span>, <span class="str">'y'</span>, <span class="str">'x'</span>, <span class="str">'y'</span>]]
idx = pd.MultiIndex.from_arrays(arrays, names=[<span class="str">'group'</span>, <span class="str">'sub'</span>])

<span class="cm"># From groupby</span>
multi_df = df.groupby([<span class="str">'dept'</span>, <span class="str">'city'</span>])[<span class="str">'salary'</span>].mean()

<span class="cm"># Access MultiIndex</span>
multi_df.loc[<span class="str">'IT'</span>]                    <span class="cm"># all IT rows</span>
multi_df.loc[(<span class="str">'IT'</span>, <span class="str">'Mumbai'</span>)]         <span class="cm"># specific</span>
multi_df.loc[pd.IndexSlice[<span class="str">'IT'</span>:, :]]  <span class="cm"># slice</span>

<span class="cm"># Flatten MultiIndex</span>
multi_df.reset_index()
df.columns = [<span class="str">'_'</span>.join(col) <span class="kw">for</span> col <span class="kw">in</span> df.columns]  <span class="cm"># for multi-level cols</span>

<span class="cm"># xs â€” cross-section</span>
multi_df.xs(<span class="str">'Mumbai'</span>, level=<span class="str">'city'</span>)`
      },
      {
        title: "Method Chaining & Pipe",
        desc: "Write readable, functional-style pipelines",
        code: `<span class="cm"># Method chaining â€” the pandas way</span>
result = (
    df
    .query(<span class="str">"region == 'South'"</span>)
    .dropna(subset=[<span class="str">'sales'</span>])
    .assign(
        tax=<span class="kw">lambda</span> x: x[<span class="str">'sales'</span>] * <span class="num">0.18</span>,
        net=<span class="kw">lambda</span> x: x[<span class="str">'sales'</span>] - x[<span class="str">'tax'</span>]
    )
    .groupby(<span class="str">'product'</span>)
    .agg(total_net=(<span class="str">'net'</span>, <span class="str">'sum'</span>), count=(<span class="str">'net'</span>, <span class="str">'count'</span>))
    .sort_values(<span class="str">'total_net'</span>, ascending=<span class="kw">False</span>)
    .head(<span class="num">10</span>)
    .reset_index()
)

<span class="cm"># pipe â€” inject custom functions into chain</span>
<span class="kw">def</span> <span class="fn">add_audit_cols</span>(df, source):
    df[<span class="str">'_source'</span>] = source
    df[<span class="str">'_loaded_at'</span>] = pd.Timestamp.now()
    <span class="kw">return</span> df

result = (
    df
    .query(<span class="str">"status == 'active'"</span>)
    .pipe(add_audit_cols, source=<span class="str">'orders_v2'</span>)
    .pipe(<span class="kw">lambda</span> x: x.dropna(thresh=int(len(x.columns) * <span class="num">0.8</span>)))
)`
      },
      {
        title: "Real-world DE Pipeline Pattern",
        desc: "Production-grade data engineering pattern",
        code: `<span class="kw">import</span> pandas <span class="kw">as</span> pd
<span class="kw">import</span> numpy <span class="kw">as</span> np
<span class="kw">from</span> functools <span class="kw">import</span> reduce

<span class="kw">def</span> <span class="fn">extract</span>(path: str) -> pd.DataFrame:
    <span class="str">"""Read raw data."""</span>
    <span class="kw">return</span> pd.read_parquet(path)

<span class="kw">def</span> <span class="fn">clean</span>(df: pd.DataFrame) -> pd.DataFrame:
    <span class="str">"""Standardize and clean."""</span>
    <span class="kw">return</span> (
        df
        .rename(columns=str.lower)
        .dropna(subset=[<span class="str">'order_id'</span>])
        .drop_duplicates(<span class="str">'order_id'</span>)
        .assign(
            order_date=<span class="kw">lambda</span> x: pd.to_datetime(x[<span class="str">'order_date'</span>]),
            amount=<span class="kw">lambda</span> x: pd.to_numeric(x[<span class="str">'amount'</span>], errors=<span class="str">'coerce'</span>)
        )
    )

<span class="kw">def</span> <span class="fn">enrich</span>(df: pd.DataFrame, lookup: pd.DataFrame) -> pd.DataFrame:
    <span class="str">"""Join dimensions."""</span>
    <span class="kw">return</span> df.merge(lookup, on=<span class="str">'product_id'</span>, how=<span class="str">'left'</span>)

<span class="kw">def</span> <span class="fn">aggregate</span>(df: pd.DataFrame) -> pd.DataFrame:
    <span class="str">"""Business metrics."""</span>
    <span class="kw">return</span> df.groupby([<span class="str">'region'</span>, <span class="str">'product'</span>]).agg(
        revenue=(<span class="str">'amount'</span>, <span class="str">'sum'</span>),
        orders=(<span class="str">'order_id'</span>, <span class="str">'count'</span>),
        aov=(<span class="str">'amount'</span>, <span class="str">'mean'</span>)
    ).reset_index()

<span class="kw">def</span> <span class="fn">load</span>(df: pd.DataFrame, dest: str) -> <span class="kw">None</span>:
    df.to_parquet(dest, index=<span class="kw">False</span>, compression=<span class="str">'snappy'</span>)
    <span class="fn">print</span>(<span class="str">f"Loaded {len(df):,} rows to {dest}"</span>)

<span class="cm"># Run pipeline</span>
orders = extract(<span class="str">"raw/orders.parquet"</span>)
products = pd.read_csv(<span class="str">"dim/products.csv"</span>)
result = orders.pipe(clean).pipe(enrich, products).pipe(aggregate)
load(result, <span class="str">"mart/order_metrics.parquet"</span>)`
      },
      {
        title: "Real-world DA Analysis Pattern",
        desc: "EDA and reporting for data analysts",
        code: `<span class="cm"># Quick EDA report</span>
<span class="kw">def</span> <span class="fn">eda_report</span>(df):
    <span class="fn">print</span>(<span class="str">f"Shape: {df.shape}"</span>)
    <span class="fn">print</span>(<span class="str">f"\nMissing:\n{df.isna().sum()[df.isna().sum() > 0]}"</span>)
    <span class="fn">print</span>(<span class="str">f"\nDuplicates: {df.duplicated().sum()}"</span>)
    <span class="fn">print</span>(<span class="str">f"\nDtypes:\n{df.dtypes}"</span>)
    <span class="kw">return</span> df.describe(include=<span class="str">'all'</span>).T

<span class="cm"># Correlation analysis</span>
corr = df.select_dtypes(<span class="str">'number'</span>).corr()
high_corr = corr[(corr > <span class="num">0.7</span>) & (corr < <span class="num">1</span>)].stack().reset_index()
high_corr.columns = [<span class="str">'feat1'</span>, <span class="str">'feat2'</span>, <span class="str">'corr'</span>]

<span class="cm"># Cohort analysis skeleton</span>
df[<span class="str">'cohort'</span>] = df.groupby(<span class="str">'user_id'</span>)[<span class="str">'order_date'</span>].transform(<span class="str">'min'</span>).dt.to_period(<span class="str">'M'</span>)
df[<span class="str">'period'</span>] = df[<span class="str">'order_date'</span>].dt.to_period(<span class="str">'M'</span>)
cohort_data = (
    df.groupby([<span class="str">'cohort'</span>, <span class="str">'period'</span>])[<span class="str">'user_id'</span>]
    .nunique()
    .reset_index()
)

<span class="cm"># Outlier detection</span>
Q1 = df[<span class="str">'col'</span>].quantile(<span class="num">0.25</span>)
Q3 = df[<span class="str">'col'</span>].quantile(<span class="num">0.75</span>)
IQR = Q3 - Q1
outliers = df[(df[<span class="str">'col'</span>] < (Q1 - <span class="num">1.5</span> * IQR)) | (df[<span class="str">'col'</span>] > (Q3 + <span class="num">1.5</span> * IQR))]`
      }
    ]
  }
];

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ NAV BUILDER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function buildNav() {
  const nav = document.getElementById('navItems');
  const categories = {};
  SECTIONS.forEach(s => {
    if (!categories[s.category]) categories[s.category] = [];
    categories[s.category].push(s);
  });

  Object.entries(categories).forEach(([cat, sections]) => {
    const catEl = document.createElement('div');
    catEl.className = 'nav-category';
    catEl.textContent = cat;
    nav.appendChild(catEl);

    sections.forEach(s => {
      const el = document.createElement('div');
      el.className = 'nav-item';
      el.id = `nav-${s.id}`;
      el.dataset.level = s.level;
      el.dataset.roles = s.roles.join(',');

      const colors = { beginner: '#3fb950', intermediate: '#ffa657', advanced: '#f78166' };
      el.innerHTML = `<span class="level-dot" style="background:${colors[s.level]}"></span>${s.icon} ${s.title}`;
      el.onclick = () => scrollTo(s.id);
      nav.appendChild(el);
    });
  });
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONTENT BUILDER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function buildContent() {
  const container = document.getElementById('content');
  SECTIONS.forEach(s => {
    const section = document.createElement('div');
    section.className = 'section';
    section.id = `section-${s.id}`;
    section.dataset.level = s.level;
    section.dataset.roles = s.roles.join(',');

    const roleBadges = s.roles.map(r => `<span class="badge ${r}">${r === 'de' ? 'Data Eng' : 'Data Analyst'}</span>`).join('');

    section.innerHTML = `
      <div class="section-header">
        <span class="section-icon">${s.icon}</span>
        <span class="section-title">${s.title}</span>
        <div class="section-meta">
          <span class="badge ${s.level}">${s.level.charAt(0).toUpperCase() + s.level.slice(1)}</span>
          ${roleBadges}
        </div>
      </div>
      <p class="description">${s.description}</p>
      ${s.cards.map((c, i) => `
        <div class="concept-card">
          <div class="card-header">
            <div>
              <div class="card-title">${c.title}</div>
              <div class="card-desc">${c.desc}</div>
            </div>
          </div>
          <div class="code-block">
            <button class="copy-btn" onclick="copyCode(this)">Copy</button>
            <pre>${c.code}</pre>
          </div>
        </div>
      `).join('')}
    `;

    container.appendChild(section);
  });
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SCROLL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function scrollTo(id) {
  document.getElementById(`section-${id}`)?.scrollIntoView({ behavior: 'smooth', block: 'start' });
  document.querySelectorAll('.nav-item').forEach(n => n.classList.remove('active'));
  document.getElementById(`nav-${id}`)?.classList.add('active');
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ COPY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function copyCode(btn) {
  const pre = btn.nextElementSibling;
  const text = pre.textContent;
  navigator.clipboard.writeText(text).then(() => {
    btn.textContent = 'Copied!';
    btn.classList.add('copied');
    setTimeout(() => { btn.textContent = 'Copy'; btn.classList.remove('copied'); }, 2000);
  });
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FILTER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
let activeLevel = 'all';
let activeRole = null;
let searchTerm = '';

function filterLevel(level, el) {
  const roleChips = ['de', 'da'];
  const levelChips = ['all', 'beginner', 'intermediate', 'advanced'];

  if (roleChips.includes(level)) {
    if (activeRole === level) {
      activeRole = null;
      el.classList.remove('active');
    } else {
      document.querySelectorAll('.chip.de, .chip.da').forEach(c => c.classList.remove('active'));
      activeRole = level;
      el.classList.add('active');
    }
  } else {
    document.querySelectorAll('.chip.all, .chip.beginner, .chip.intermediate, .chip.advanced').forEach(c => c.classList.remove('active'));
    activeLevel = level;
    el.classList.add('active');
  }
  applyFilters();
}

function filterSearch(val) {
  searchTerm = val.toLowerCase();
  applyFilters();
}

function applyFilters() {
  SECTIONS.forEach(s => {
    const section = document.getElementById(`section-${s.id}`);
    const navItem = document.getElementById(`nav-${s.id}`);

    const levelOk = activeLevel === 'all' || s.level === activeLevel;
    const roleOk = !activeRole || s.roles.includes(activeRole);
    const searchOk = !searchTerm ||
      s.title.toLowerCase().includes(searchTerm) ||
      s.description.toLowerCase().includes(searchTerm) ||
      s.cards.some(c => c.title.toLowerCase().includes(searchTerm) || c.desc.toLowerCase().includes(searchTerm));

    const visible = levelOk && roleOk && searchOk;
    section.style.display = visible ? '' : 'none';
    navItem.style.display = visible ? '' : 'none';
  });
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SCROLL SPY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const main = document.getElementById('main');
main.addEventListener('scroll', () => {
  const scrollTop = main.scrollTop;
  const scrollHeight = main.scrollHeight - main.clientHeight;
  const progress = (scrollTop / scrollHeight) * 100;
  document.getElementById('progress-bar').style.width = progress + '%';

  let active = null;
  SECTIONS.forEach(s => {
    const el = document.getElementById(`section-${s.id}`);
    if (el && el.offsetTop - 80 <= scrollTop) active = s.id;
  });

  if (active) {
    document.querySelectorAll('.nav-item').forEach(n => n.classList.remove('active'));
    document.getElementById(`nav-${active}`)?.classList.add('active');
  }
});

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ INIT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
buildNav();
buildContent();
</script>
</body>
</html>